{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"julia-1.0","display_name":"Julia 1.0"},"accelerator":"GPU","colab":{"name":"10.01 – Embeddings","provenance":[{"file_id":"10n2JYjUZPo_WQPNDCf6LkCsTeXIHhjg9","timestamp":1573477803903},{"file_id":"1SvBM6AiSQ-TxddhLnaF0mcUYdtvJUXAp","timestamp":1573463816210},{"file_id":"1dvrhWR_ObIYVs9pxlXW-jGBRGVp2tQX1","timestamp":1572543467412},{"file_id":"1W_y94cY787aFyfugf7Nmty6Tm_3JrN66","timestamp":1571838794136},{"file_id":"1gMhnrMNP-het4rexXLXHzaOytfDpUxgR","timestamp":1571838737616}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","outputId":"b80e5c3f-96b0-4c3c-e1ac-2138f0ea5cc9","executionInfo":{"status":"ok","timestamp":1573478083286,"user_tz":0,"elapsed":2987,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"id":"AXbVZCfh5Olh","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Installation cell\n","%%shell\n","if ! command -v julia 2>&1 > /dev/null\n","then\n","    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz' \\\n","        -O /tmp/julia.tar.gz\n","    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n","    rm /tmp/julia.tar.gz\n","fi\n","julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/latex":"Unrecognized magic \\texttt{\\%\\%shell}.\n\nJulia does not use the IPython \\texttt{\\%magic} syntax.   To interact with the IJulia kernel, use \\texttt{IJulia.somefunction(...)}, for example.  Julia macros, string macros, and functions can be used to accomplish most of the other functionalities of IPython magics.\n\n","text/markdown":"Unrecognized magic `%%shell`.\n\nJulia does not use the IPython `%magic` syntax.   To interact with the IJulia kernel, use `IJulia.somefunction(...)`, for example.  Julia macros, string macros, and functions can be used to accomplish most of the other functionalities of IPython magics.\n","text/plain":["  Unrecognized magic \u001b[36m%%shell\u001b[39m.\n","\n","  Julia does not use the IPython \u001b[36m%magic\u001b[39m syntax. To interact with the IJulia\n","  kernel, use \u001b[36mIJulia.somefunction(...)\u001b[39m, for example. Julia macros, string\n","  macros, and functions can be used to accomplish most of the other\n","  functionalities of IPython magics."]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9uKBM4zJ5OlX","outputId":"19cd7184-0c4f-412a-cf82-c3210905df0c","executionInfo":{"status":"ok","timestamp":1573478298156,"user_tz":0,"elapsed":211980,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["using Pkg\n","\n","pkg\"add Embeddings; precompile;\"\n","using Embeddings\n","\n","pkg\"add Distances; precompile;\"\n","using Distances\n","\n","const w2v = load_embeddings(Word2Vec)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n","\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n","\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m AutoHashEquals ─ v0.2.0\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Reexport ─────── v0.2.0\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m IniFile ──────── v0.5.0\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Embeddings ───── v0.3.1\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DataDeps ─────── v0.7.0\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m HTTP ─────────── v0.8.7\n","\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n"," \u001b[90m [c5bfea45]\u001b[39m\u001b[92m + Embeddings v0.3.1\u001b[39m\n","\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n"," \u001b[90m [15f4f7f2]\u001b[39m\u001b[92m + AutoHashEquals v0.2.0\u001b[39m\n"," \u001b[90m [124859b0]\u001b[39m\u001b[92m + DataDeps v0.7.0\u001b[39m\n"," \u001b[90m [c5bfea45]\u001b[39m\u001b[92m + Embeddings v0.3.1\u001b[39m\n"," \u001b[90m [cd3eb016]\u001b[39m\u001b[92m + HTTP v0.8.7\u001b[39m\n"," \u001b[90m [83e8ac13]\u001b[39m\u001b[92m + IniFile v0.5.0\u001b[39m\n"," \u001b[90m [189a3867]\u001b[39m\u001b[92m + Reexport v0.2.0\u001b[39m\n","\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n","\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m Embeddings\n"],"name":"stdout"},{"output_type":"stream","text":["┌ Info: Precompiling Embeddings [c5bfea45-b7f1-5224-a596-15500f5db411]\n","└ @ Base loading.jl:1192\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n","\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Distances ─ v0.8.2\n","\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n"," \u001b[90m [b4f34e82]\u001b[39m\u001b[92m + Distances v0.8.2\u001b[39m\n","\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n"," \u001b[90m [b4f34e82]\u001b[39m\u001b[92m + Distances v0.8.2\u001b[39m\n","\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n","\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m Distances\n"],"name":"stdout"},{"output_type":"stream","text":["┌ Info: Precompiling Distances [b4f34e82-e78d-54a5-968a-f98e89d6e8f7]\n","└ @ Base loading.jl:1192\n"],"name":"stderr"},{"output_type":"stream","text":["This program has requested access to the data dependency word2vec 300d.\n","which is not currently installed. It can be installed automatically, and you will not see this message again.\n","\n","Pretrained Word2Vec Word emeddings\n","Website: https://code.google.com/archive/p/word2vec/\n","Author: Mikolov et al.\n","Year: 2013\n","\n","Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n","\n","Paper:\n","    Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n","\n","\n","\n","Do you want to download the dataset from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz to \"/root/.julia/datadeps/word2vec 300d\"?\n","[y/n]\n","stdin> y\n"],"name":"stdout"},{"output_type":"stream","text":["┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.1036\n","│   time_taken = 5.0 s\n","│   time_remaining = 43.25 s\n","│   average_speed = 32.555 MiB/s\n","│   downloaded = 162.807 MiB\n","│   remaining = 1.375 GiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.2165\n","│   time_taken = 10.0 s\n","│   time_remaining = 36.19 s\n","│   average_speed = 34.005 MiB/s\n","│   downloaded = 340.119 MiB\n","│   remaining = 1.202 GiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.3339\n","│   time_taken = 15.0 s\n","│   time_remaining = 29.92 s\n","│   average_speed = 34.962 MiB/s\n","│   downloaded = 524.541 MiB\n","│   remaining = 1.022 GiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.4515\n","│   time_taken = 20.0 s\n","│   time_remaining = 24.31 s\n","│   average_speed = 35.447 MiB/s\n","│   downloaded = 709.119 MiB\n","│   remaining = 861.626 MiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.5688\n","│   time_taken = 25.01 s\n","│   time_remaining = 18.95 s\n","│   average_speed = 35.731 MiB/s\n","│   downloaded = 893.478 MiB\n","│   remaining = 677.267 MiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.6857\n","│   time_taken = 30.01 s\n","│   time_remaining = 13.75 s\n","│   average_speed = 35.895 MiB/s\n","│   downloaded = 1.052 GiB\n","│   remaining = 493.642 MiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.8031\n","│   time_taken = 35.01 s\n","│   time_remaining = 8.58 s\n","│   average_speed = 36.035 MiB/s\n","│   downloaded = 1.232 GiB\n","│   remaining = 309.220 MiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 0.9201\n","│   time_taken = 40.01 s\n","│   time_remaining = 3.48 s\n","│   average_speed = 36.121 MiB/s\n","│   downloaded = 1.411 GiB\n","│   remaining = 125.564 MiB\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","┌ Info: Downloading\n","│   source = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","│   dest = /root/.julia/datadeps/word2vec 300d/GoogleNews-vectors-negative300.bin.gz\n","│   progress = 1.0\n","│   time_taken = 43.43 s\n","│   time_remaining = 0.0 s\n","│   average_speed = 36.170 MiB/s\n","│   downloaded = 1.534 GiB\n","│   remaining = 0 bytes\n","│   total = 1.534 GiB\n","└ @ HTTP /root/.julia/packages/HTTP/6Smhf/src/download.jl:119\n","tcmalloc: large alloc 3600007168 bytes == 0xcbfa000 @  0x7fe7e5f19b6b 0x7fe7e5f39379 0x7fe7e582c5ec 0x7fe7e57f642b 0x7fe7c8449f17 0x7fe7c8448eac 0x7fe7e57d505c 0x7fe7e57d9ca7 0x7fe7c8448a08 0x7fe7c8448763 0x7fe7c8448848 0x7fe7e57d9ca7 0x7fe7c8415719 0x7fe7c84157db 0x7fe7e57d9ca7 0x7fe7c84154ba 0x7fe7e57d90d6 0x7fe7e5942f10 0x7fe7e5942c39 0x7fe7e59435bc 0x7fe7e5943d3f 0x7fe7e57ef96c 0x7fe7e594480d 0x7fe7e580effc 0x7fe7e57ea8e0 0x7fe7d3704a44 0x7fe7d370238d 0x7fe7e57d90d6 0x7fe7e57e7846 0x7fe7e57e7ea2 0x7fe7d36f83d9\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.0673199 0.0529562 … -0.21143 0.0136373; -0.0534466 0.0654598 … -0.0087888 -0.0742876; … ; -0.00733469 0.0108946 … -0.00405157 0.0156112; -0.00514565 -0.0470722 … -0.0341579 0.0396559], [\"</s>\", \"in\", \"for\", \"that\", \"is\", \"on\", \"##\", \"The\", \"with\", \"said\"  …  \"#-###-PA-PARKS\", \"Lackmeyer\", \"PERVEZ\", \"KUNDI\", \"Budhadeb\", \"Nautsch\", \"Antuane\", \"tricorne\", \"VISIONPAD\", \"RAFFAELE\"])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"900dqb9wKoXJ","colab_type":"text"},"source":["# Embeddings #\n","\n","Up until this point we have dealt with continuous data (images, measurements, etc.), but a healthy amount of data one can be interested in is discrete. Think of graph structures, language, etc. These pose a challenge for neural models, why?\n","\n","The vocabulary of a model for language is frequently in the millions and a big knowledge graph like Wikidata contains [66,689,671](https://www.wikidata.org/wiki/Wikidata:Statistics) entities.\n","\n","What does a good representation look like in an abstract sense?:\n","\n","1. Representations are **distinct**\n","\n","2. Similar items have **similar** representations\n","\n","If both of these statements hold, it is likely that a representation can be useful for a downstream machine learning system."]},{"cell_type":"markdown","metadata":{"id":"UsxD0VxOQjPN","colab_type":"text"},"source":["## An indicator function ##\n","\n","For more “traditional” machine learning systems, we are likely to use an indicator function such as:\n","\n","$$f_{id}(w) \\mapsto \\mathbb{N^{*}}$$\n","\n","We can then create a sparse binary representation using this function:\n","\n","$$g(w, i) = {\\left\\lbrace\n","                            \\begin{array}{ll}\n","                                1 & \\textrm{if }~i = f_{id}(w) \\\\\n","                                0 & \\textrm{otherwise} \\\\\n","                            \\end{array}\\right.}$$\n","\n","$$f_{sb}(w) = [g(w, 1), \\ldots]$$\n"]},{"cell_type":"markdown","metadata":{"id":"F5U4A4ObR36G","colab_type":"text"},"source":["As a concrete “toy” example, we can imagine:\n","\n","$$\\mathbb{V} = \\{\\textrm{cat}, \\textrm{dog}, \\textrm{human}\\}$$\n","\n","$$ f_{id}(\\textrm{cat}) = 1, \\ldots, f_{id}(\\textrm{human}) = 3$$\n","\n","$$f_{sb}(\\textrm{cat}) = [1, 0, 0]$$\n","\n","$$f_{sb}(\\textrm{dog}) = [0, 1, 0]$$\n","\n","$$f_{sb}(\\textrm{human}) = [0, 1, 0]$$"]},{"cell_type":"markdown","metadata":{"id":"wtEZsNsOn46j","colab_type":"text"},"source":["A big problem with this approach is that there's no distinction between how similar things are. I.e. everything is orthogonal and cat/dog/human are all equally similar / different"]},{"cell_type":"markdown","metadata":{"id":"cKkQojUW182j","colab_type":"text"},"source":["## A case study: word2vec ##\n","\n","**Cbow representation**: fill in the blanked word based on the window around it\n","\n","**Skip-gram**: predict all words in the window based on $w(t)$\n","\n","![](https://cdn-images-1.medium.com/max/1600/1*GkL9XH1XXSUU0RfAHKJGCg.png)\n","\n","[Image source](https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf)\n","\n","“If A and B have almost identical environments we say that they are synonyms” – Zellig Harris (1954)\n","\n","“You shall know a word by the company it keeps” – John Rupert Firth (1957)\n","\n","For some intuition, consider: “I had some _____ for breakfast today”, you instinctively know that “airplanes” is likely to be wrong, but “toast” is likely to be right.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AjVP6OxVF8BT","colab_type":"text"},"source":["### Formalised ###\n","\n","$$ W^w \\in \\mathbb{R}^{|\\mathbb{V}| \\times d} $$\n","\n","$$ w \\in \\mathbb{V} $$\n","\n","$$ W^c \\in \\mathbb{R}^{|\\mathbb{V}| \\times d} $$\n","\n","$$ c \\in \\mathbb{V} $$\n","\n","$$ σ(x)  = \\frac{1}{1 + e^{-x}} $$\n","\n","$$ \\mathcal{D} = \\{(w_1, c_1), \\ldots, (w_n, c_n)\\} $$\n","\n","$$ \\mathcal{D}^{\\prime} = \\{(w_1, c_1), \\ldots, (w_m, c_m)\\} $$\n","\n","$$ p((c, w) \\in \\mathbb{D}|c, w, W^{w}, W^{c}) = \\sigma(W^{c}_{f_{id}(c), :} \\cdot W^{w}_{f_{id}(w), :}) $$\n","\n","$$ arg\\max_{w,b}\n","    \\sum_{(w, c) \\in D} \\textrm{log}(W^{c}_{f_{id}(c), :} \\cdot W^{w}_{f_{id}(w), :})\n","    + \\sum_{(w, c) \\in D^{\\prime}} \\textrm{log}(-W^{c}_{f_{id}(c), :} \\cdot W^{c}_{f_{id}(w), :})\n","$$\n","\n","Why is $\\mathcal{D}^{\\prime}$ necessary? What happens if we remove it?\n","\n","The $\\mathcal{D}^{\\prime}$ stuff is referred to as *noise contrasted optimisation*"]},{"cell_type":"code","metadata":{"id":"7G_Gp558x3yq","colab_type":"code","outputId":"f595643b-2a30-4392-9458-a268881ee76e","executionInfo":{"status":"ok","timestamp":1573479372054,"user_tz":0,"elapsed":2087,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["const token2index = Dict(token => i for (i, token) in enumerate(w2v.vocab))\n","embedding(token)  = w2v.embeddings[:, token2index[token]]"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["embedding (generic function with 1 method)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"uvsW0jq8NPip","colab_type":"text"},"source":["### Cosine distance ###\n","\n","$$ cos(u, v) = 1 - \\frac{u \\cdot v}{||u|| ||v||} $$\n","\n","$$ cos(u, v) \\mapsto [0, 2] $$\n","\n","$$ cos(u, v) = 0 \\implies \\textrm{identical} $$\n","\n","$$ cos(u, v) = 2 \\implies \\textrm{opposites} $$\n","\n","$$ cos(u, v) = 1 \\implies \\textrm{orthogonal} $$\n","\n","Why are we not using something more “standard” like the Euclidian distance?"]},{"cell_type":"code","metadata":{"id":"sPQ6Rwzo2Jrf","colab_type":"code","outputId":"85dc0f12-90e5-4eb7-b1fa-6546c91bd515","executionInfo":{"status":"ok","timestamp":1573479588462,"user_tz":0,"elapsed":696,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["distance(tokena, tokenb) = cosine_dist(embedding(tokena), embedding(tokenb))\n","\n","@show distance(\"dog\", \"cat\")\n","@show distance(\"cat\", \"dog\") # Symmetric, of course.\n","@show distance(\"cat\", \"human\")\n","@show distance(\"dog\", \"human\")\n","@show distance(\"ape\", \"human\");"],"execution_count":4,"outputs":[{"output_type":"stream","text":["distance(\"dog\", \"cat\") = 0.2390545f0\n","distance(\"cat\", \"dog\") = 0.2390545f0\n","distance(\"cat\", \"human\") = 0.75563335f0\n","distance(\"dog\", \"human\") = 0.8109204f0\n","distance(\"ape\", \"human\") = 0.6500703f0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u8NY54-r_5qY","colab_type":"text"},"source":["![](https://cdn-images-1.medium.com/max/1600/1*vvtIsW1AblmgLkq1peKfOg.png)\n","\n","[Image source](https://towardsdatascience.com/mapping-word-embeddings-with-word2vec-99a799dc9695)"]},{"cell_type":"markdown","metadata":{"id":"dwJwp4ua2oC3","colab_type":"text"},"source":["## Neighbourhoods ##"]},{"cell_type":"code","metadata":{"id":"ve2Q_scR1NFF","colab_type":"code","outputId":"290c462c-e3c7-4666-de1a-8ac30e546b56","executionInfo":{"status":"ok","timestamp":1573479927786,"user_tz":0,"elapsed":1050,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["function neighbourhood(emb; n=32)\n","    nneighbours = Int[]\n","    maxdist     = typemax(Float32)\n","    for i in eachindex(w2v.vocab)\n","        d = cosine_dist(emb, w2v.embeddings[:, i])\n","        if length(nneighbours) > n && d >= maxdist\n","            continue\n","        end\n","        push!(nneighbours, i)\n","        sort!(nneighbours,\n","            by=j -> cosine_dist(emb, w2v.embeddings[:, j]))\n","        maxdist = cosine_dist(emb, w2v.embeddings[:, nneighbours[end]])\n","        length(nneighbours) <= n || pop!(nneighbours)\n","    end\n","    [(w2v.vocab[i], cosine_dist(emb, w2v.embeddings[:, i]))\n","        for i in nneighbours]\n","end\n","neighbourhood(token::String; n=32) = neighbourhood(embedding(token), n=n)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neighbourhood (generic function with 2 methods)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"yn45Jdur2nqv","colab_type":"code","outputId":"65d533c3-00f6-4532-f190-20ae9efe3a52","executionInfo":{"status":"ok","timestamp":1573480045351,"user_tz":0,"elapsed":112741,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["neighbourhood(\"religion\")"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32-element Array{Tuple{String,Float32},1}:\n"," (\"religion\", 0.0)            \n"," (\"religions\", 0.2513935)     \n"," (\"religious\", 0.27735394)    \n"," (\"Christianity\", 0.29210013) \n"," (\"Religion\", 0.34160268)     \n"," (\"relgion\", 0.35627824)      \n"," (\"religon\", 0.36557502)      \n"," (\"Islam\", 0.37923366)        \n"," (\"faiths\", 0.38649082)       \n"," (\"religous\", 0.40177977)     \n"," (\"spirituality\", 0.40435177) \n"," (\"religiosity\", 0.4094324)   \n"," (\"morality\", 0.41415274)     \n"," ⋮                            \n"," (\"religiousness\", 0.43363798)\n"," (\"Hinduism\", 0.43809837)     \n"," (\"Buddism\", 0.43927014)      \n"," (\"Judaism\", 0.44100153)      \n"," (\"teachings\", 0.44211966)    \n"," (\"Paganism\", 0.44220132)     \n"," (\"Universalism\", 0.44323426) \n"," (\"secularism\", 0.44477552)   \n"," (\"Buddhism\", 0.44490796)     \n"," (\"nonreligion\", 0.4462548)   \n"," (\"creed\", 0.44685173)        \n"," (\"ethnicity\", 0.4470787)     "]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"4IOZZkIS93fY","colab_type":"code","outputId":"97613c18-891e-48e4-cb57-83b454043a72","executionInfo":{"status":"ok","timestamp":1573467607780,"user_tz":0,"elapsed":181786,"user":{"displayName":"Pontus Stenetorp","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnmSzpEb4daBaXfgjnYgCFcuWRI9oi8kHJYLofgQ=s64","userId":"17008380506712831670"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["neighbourhood(\"england\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32-element Array{Tuple{String,Float32},1}:\n"," (\"england\", 0.0)         \n"," (\"liverpool\", 0.28275466)\n"," (\"chelsea\", 0.29395294)  \n"," (\"fulham\", 0.29792225)   \n"," (\"tottenham\", 0.3092957) \n"," (\"rooney\", 0.3136869)    \n"," (\"torres\", 0.32362872)   \n"," (\"ronaldo\", 0.3242663)   \n"," (\"spain\", 0.32593936)    \n"," (\"gerrard\", 0.32909578)  \n"," (\"anelka\", 0.3319348)    \n"," (\"fergie\", 0.3326596)    \n"," (\"europe\", 0.335052)     \n"," ⋮                        \n"," (\"newcastle\", 0.3432166) \n"," (\"leeds\", 0.34379578)    \n"," (\"barca\", 0.34468734)    \n"," (\"madrid\", 0.3449734)    \n"," (\"everton\", 0.34627277)  \n"," (\"juve\", 0.3468153)      \n"," (\"wenger\", 0.34718215)   \n"," (\"benitez\", 0.3498447)   \n"," (\"beckham\", 0.3502348)   \n"," (\"henry\", 0.35096765)    \n"," (\"italy\", 0.3517356)     \n"," (\"pompey\", 0.35407293)   "]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"bEzm66Wtziyt","colab_type":"text"},"source":["### Word representation algebra ###\n","\n","$$ f_{n}(\\textrm{king}) - f_{n}(\\textrm{man}) + f_{n}(\\textrm{woman}) \\approx f_{n}(\\textrm{queen}) $$\n","\n","$$ f_{n}(\\textrm{Paris}) - f_{n}(\\textrm{France}) + f_{n}(\\textrm{Italy}) \\approx f_{n}(\\textrm{Rome})$$\n","\n","This is notoriously difficult to replicate though…"]},{"cell_type":"code","metadata":{"id":"nmv_HywN9-JX","colab_type":"code","outputId":"980764b3-2a4c-4f12-af0b-0ae41b7a883e","executionInfo":{"status":"ok","timestamp":1573480169241,"user_tz":0,"elapsed":1071,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["isto(ais, tob, likecto) = neighbourhood(embedding(ais) .- embedding(tob) .+ embedding(likecto))"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["isto (generic function with 1 method)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"6-Uyr7cm-yH2","colab_type":"code","outputId":"f6d0b149-0294-4ab4-a9f4-cd417f7f4fb8","executionInfo":{"status":"ok","timestamp":1573480290130,"user_tz":0,"elapsed":110992,"user":{"displayName":"Louis Vines","photoUrl":"","userId":"03946309289332150370"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["isto(\"king\", \"man\", \"woman\")"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32-element Array{Tuple{String,Float32},1}:\n"," (\"king\", 0.2007401)         \n"," (\"queen\", 0.28818095)       \n"," (\"monarch\", 0.3810326)      \n"," (\"princess\", 0.4097572)     \n"," (\"prince\", 0.462268)        \n"," (\"kings\", 0.4763158)        \n"," (\"queens\", 0.4818864)       \n"," (\"sultan\", 0.4901408)       \n"," (\"monarchy\", 0.49125886)    \n"," (\"throne\", 0.49941933)      \n"," (\"royal\", 0.5061796)        \n"," (\"ruler\", 0.50907254)       \n"," (\"empress\", 0.5112186)      \n"," ⋮                           \n"," (\"Mswati\", 0.5552143)       \n"," (\"maharaja\", 0.55632824)    \n"," (\"princesses\", 0.5576925)   \n"," (\"Princess\", 0.5624594)     \n"," (\"duchess\", 0.5627598)      \n"," (\"Queen\", 0.5653622)        \n"," (\"monarchs\", 0.5656772)     \n"," (\"handmaid\", 0.5685401)     \n"," (\"Gosakuramachi\", 0.5699681)\n"," (\"kumari\", 0.57787603)      \n"," (\"maharani\", 0.57856727)    \n"," (\"commoner\", 0.57872516)    "]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"0PNfVP1M_XHh","colab_type":"code","outputId":"bff04f1c-4816-4d80-fa26-fa55811f5ba1","executionInfo":{"status":"ok","timestamp":1573468168300,"user_tz":0,"elapsed":340101,"user":{"displayName":"Pontus Stenetorp","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDnmSzpEb4daBaXfgjnYgCFcuWRI9oi8kHJYLofgQ=s64","userId":"17008380506712831670"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["isto(\"Paris\", \"France\", \"Italy\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32-element Array{Tuple{String,Float32},1}:\n"," (\"Milan\", 0.27778602)  \n"," (\"Rome\", 0.2971689)    \n"," (\"Italy\", 0.3267352)   \n"," (\"Paris\", 0.3448093)   \n"," (\"Italian\", 0.4088726) \n"," (\"Tuscany\", 0.43671834)\n"," (\"Bologna\", 0.43916416)\n"," (\"Sicily\", 0.44036168) \n"," (\"Genoa\", 0.46910983)  \n"," (\"Milanese\", 0.4694308)\n"," (\"Rimini\", 0.48378998) \n"," (\"Venice\", 0.4858547)  \n"," (\"ANSA\", 0.4889533)    \n"," ⋮                      \n"," (\"Pisa\", 0.499672)     \n"," (\"Liguria\", 0.5008726) \n"," (\"Sassari\", 0.50194716)\n"," (\"Brianza\", 0.50275654)\n"," (\"Sempione\", 0.5056912)\n"," (\"Pietro\", 0.5061012)  \n"," (\"Pomezia\", 0.50707614)\n"," (\"Giorgio\", 0.5097326) \n"," (\"Vicenza\", 0.5102454) \n"," (\"Gianni\", 0.51183736) \n"," (\"Pistoia\", 0.51197916)\n"," (\"Jacopo\", 0.51241815) "]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"rXGae9WbBC-U","colab_type":"text"},"source":["![](https://samyzaf.com/ML/nlp/word2vec2.png)\n","\n","[Image source](https://samyzaf.com/ML/nlp/nlp.html)\n","\n","Why and how would these relationships be preserved in the embeddings? Think, what do they capture?"]},{"cell_type":"markdown","metadata":{"id":"rg6RxeA8BMhD","colab_type":"text"},"source":["**Beware** when playing around with these models though, they are a reflection of the underlying data and very well mirrors the worst of us as a species in our racism, sexism, etc."]},{"cell_type":"markdown","metadata":{"id":"ffnmf-Lo0Szd","colab_type":"text"},"source":["## Some closing comments and questions… ##\n","\n","The same line of thinking case be used for graph embeddings, etc. I also find it helpful to think about what an MLP or CNN achieves in a similar way.\n","\n","1. How do we handle sequences?\n","\n","2. Our representations are context independent, is this a problem?\n","\n","3. Can we imagine loss functions akin to word2vec for other areas?\n","\n","4. From your Linear Algebra, does $W^w$ and $W^c$ remind you of something? Does this hint at another way to learn embeddings?"]},{"cell_type":"markdown","metadata":{"id":"fix-wADHz8wP","colab_type":"text"},"source":["## Extracurricular reading ##\n","\n","1. [“Distributed Representations of Words and Phrases and their Compositionality”](https://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases) by Mikolov et al. (2013)\n","\n","2. [“word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method”](https://arxiv.org/abs/1402.3722) by Goldberg and Levy (2014). I *highly* recommend to **read this in parallel with Mikolov et al. (2013)**."]},{"cell_type":"code","metadata":{"id":"HWdxUSGfn2hF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}