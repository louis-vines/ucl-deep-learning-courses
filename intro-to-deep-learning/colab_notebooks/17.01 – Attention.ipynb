{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"julia-1.0","display_name":"Julia 1.0"},"accelerator":"GPU","colab":{"name":"17.01 – Attention","provenance":[{"file_id":"1UEDj50b9k4O6bsck0miSo0GU-D396mnZ","timestamp":1599156780122},{"file_id":"1QEyJVaLQJi1lrhd3MrS3j_I2j2v8CLuR","timestamp":1574862159690},{"file_id":"1PtxJeS3_CmX2xSZpmwhtYNlFQVfwk9vM","timestamp":1574769624475},{"file_id":"1oMPEqUhIyCphVCexXbkauuytJWI2GdlA","timestamp":1574289127295},{"file_id":"1bCs7QhD_ZwswH-dMbVoJHb5WbDdfhi_X","timestamp":1573572226314},{"file_id":"10n2JYjUZPo_WQPNDCf6LkCsTeXIHhjg9","timestamp":1573560458651},{"file_id":"1SvBM6AiSQ-TxddhLnaF0mcUYdtvJUXAp","timestamp":1573463816210},{"file_id":"1dvrhWR_ObIYVs9pxlXW-jGBRGVp2tQX1","timestamp":1572543467412},{"file_id":"1W_y94cY787aFyfugf7Nmty6Tm_3JrN66","timestamp":1571838794136},{"file_id":"1gMhnrMNP-het4rexXLXHzaOytfDpUxgR","timestamp":1571838737616}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"900dqb9wKoXJ","colab_type":"text"},"source":["# Attention #"]},{"cell_type":"markdown","metadata":{"id":"DRxcCwyRHDBB","colab_type":"text"},"source":["## Revisiting the recurrent neural network ##\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n","\n","A useful perspective on recurrent neural networks is as we have discussed before, to see it as a very deep feed-forward neural network – think multi-layer perceptron – where the same weights are used at each layer.\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)\n","\n","In terms of a learning problem, it becomes gradually more difficult to learn relationships as the distance between the related input and output grows.\n","\n","![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n","\n","\n","The LSTM with its cell state does resolve the issue of vanishing gradients, but as we are about to see, challenges still remain when it comes to long-term dependencies."]},{"cell_type":"markdown","metadata":{"id":"zbkXm4raLqJ4","colab_type":"text"},"source":["## Sequence-to-Sequence recurrent neural network (seq2seq) ##\n","\n","![](https://d3i71xaburhd42.cloudfront.net/cd52da21cdec50b25b6fb0ba6741091ad38fc986/2-Figure1-1.png)\n","\n","(From Sutskever et al. (2014))\n","\n","![](https://i.stack.imgur.com/Ux0Xh.png)\n","\n","(From Cho et al. (2014))\n","\n","Very briefly at the end of the recurrent neural network lecture last week I presented this neural network structure commonly referred to as “seq2seq”. It was actually discovered independently by two different groups back in 2014. Sutskever et al. (2014) at Google Brain and Cho et al. (2014) at Mila in Montréal – the latter being the original GRU paper.\n","\n","Here is a concrete realisation of this architecture for the task it was originally designed for: machine translation.\n","\n","![](https://cdn-images-1.medium.com/max/1600/1*iXV7BD2iKMBIG-tmyzx88g.jpeg)\n","\n","Formall, we have two functions. An *encoder* that takes a sequence of discrete symbols (or discrete symbols encoded as vectors, remember word2vec) and turns into into a low-dimensional representation of its information content:\n","\n","$$ f_{enc}(X) = \\textbf{h} $$\n","\n","Which it then passes to a decoder function that based on $\\textbf{h}$ decodes an output sequence $Y^{\\prime}$:\n","\n","$$ f_{dec}(\\textbf{h}) = Y^{\\prime} $$\n","\n","In a way, if $X = Y$, you can think of these as autoencoders with discrete, variable-length inputs and outputs. Not entirely surprisingly, they have been pre-trained in exactly this way (Dai and Le, 2015).\n","\n","Considering the model from a perspective we are already familiar with, we can see the encoder as nothing more than what we have already spoken about in terms of using a recurrent neural network to encode a variable-width sequence as a fixed-size output. The decoder component is performing language modelling akin to what we have already studied, the only real difference is that language modelling *conditioned* on a hidden state as well as what has already been generated.\n"]},{"cell_type":"markdown","metadata":{"id":"o1g8ABVcTqcp","colab_type":"text"},"source":["## Alignment in machine translation ##\n","\n","However, in 2014 these initial seq2seq models could not compete with “oldschool” statistical machine translation models. Rather, they were seen as proof of concepts that may one day lead to neural models being able to perform high quality translation.\n","\n","![](https://miro.medium.com/max/1522/1*VMsuEe0XNzi2WGxKMnHgew.png)\n","\n","In machine translation it is actually not that common to simply associate an input sequence with an output sequence. Rather, you also *align* input tokens and output tokens. This dates back to at least the 80s with the very first “modern” statistical machine translation models. So, can we draw upon this structure to construct a superior seq2seq model?"]},{"cell_type":"markdown","metadata":{"id":"z2yca5w3afee","colab_type":"text"},"source":["## seq2seq with attention ##\n","\n","A common pattern in this course has been to take something non-differentiabel with inspiration from previous work or the natural world, what would happen if we did the same thing to the fact that oldschool statistical machine translation models rely on alignment?\n","\n","![](https://user-images.githubusercontent.com/7529838/31751822-86b68320-b4c2-11e7-8f19-165a5ec4c021.png)\n","\n","We have an encoded input sequence:\n","\n","$$ \\textbf{h}^{enc} = \\{ h^{enc}_1, \\ldots, h^{enc}_t \\} $$\n","\n","At each time step in the *decoder*, we take the dot product between our output state and *each* encoded input:\n","\n","$$ \\textbf{a} = \\textrm{softmax}(\\{h^{dec}_i \\cdot h^{enc}_1, \\ldots,  h^{dec}_t \\cdot h^{enc}_t\\}) $$\n","\n","This gives us a probability distribution over $\\textbf{h}^{enc}$ which we can use to scale the contribution of each member of $\\textbf{h}^{enc}$:\n","\n","$$ \\sum_{i = 1}^{t}  a_i h^{enc}_i $$\n","\n","We then use this resulting vector when predicting the next token to output by the decoder.\n","\n","Let us go back to that alignment picture and see how one can interpret it in a new light, that of “soft alignments”.\n","\n","This seemingly simple addition to the seq2seq model by Bahdanau et al. (2015) led to neural machine translation subsuming the field until the present day."]},{"cell_type":"markdown","metadata":{"id":"hBo3oTvDGrnK","colab_type":"text"},"source":["## Decomposable attention ##\n","\n","![](https://miro.medium.com/max/1200/1*ZByTGTbUbqmwyr8M1cAVjg.png)\n","\n","(From [Parikh et al. (2016)](https://www.aclweb.org/anthology/D16-1244.pdf))\n","\n","Attention at this stage was looked at fairly cynically by myself and others as addressing the difficulty of training a model capable of encoding a long sequence into a single vector and then unpacking it – I stick to this view to this day. But a year later the Google Brain group sought to build a *pure* attention-based model, *without any recurrence*.\n","\n","For their task, they had two sequences and was asked to provide a classification decision. What they opted to do was to calculate the attention between each word, then aggregate these interactions, and lastly feed it into a multi-layer perceptron – simple! What was amazing were the results, this model performed as well as models an order of magnitude more weights! This was a shot across the bow against the recurrent neural networks that had dominated natural language processing for the last three years at that point.\n","\n","Can we relate this model to the “bag of vectors” model from Assignment 2 somehow?"]},{"cell_type":"markdown","metadata":{"id":"9BTfoTlnGyVt","colab_type":"text"},"source":["## The transformer ##\n","\n","As a next logical step, the same group at Google Brain then moved to introduce weights into the attention mechanism, leading to the model we now commonly know as the transformer.\n","\n","![](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n","\n","![](https://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n","\n","A key point to not here, the weights for the feed-forward network are shared.\n","\n","![](https://jalammar.github.io/images/t/transformer_self_attention_score.png)\n","\n","![](https://jalammar.github.io/images/t/self-attention_softmax.png)\n","\n","![](https://jalammar.github.io/images/t/self-attention-output.png)\n","\n","![](https://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n","\n","![](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n","\n","![](https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)\n","\n","![](https://jalammar.github.io/images/t/transformer_attention_heads_z.png)\n","\n","![](https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)\n","\n","![](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n","\n","If we manually inpsect the attention heads, we quickly \n","\n","![](https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png)\n","\n","So what about the decoder then… It actually uses the same kind of self-attention units and decodes \n","\n","![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)"]},{"cell_type":"markdown","metadata":{"id":"IXN-WGevosXb","colab_type":"text"},"source":["## Acknowledgements ##\n","\n","[Jay Alammar’s](https://jalammar.github.io/illustrated-transformer) wonderful “The Illustrated Transformer”."]}]}