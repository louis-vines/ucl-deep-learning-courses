{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"julia-1.0","display_name":"Julia 1.0"},"accelerator":"GPU","colab":{"name":"08.01 - Multilayer perceptrons and backpropagation","provenance":[{"file_id":"1dvrhWR_ObIYVs9pxlXW-jGBRGVp2tQX1","timestamp":1573724876328},{"file_id":"1W_y94cY787aFyfugf7Nmty6Tm_3JrN66","timestamp":1571838794136},{"file_id":"1gMhnrMNP-het4rexXLXHzaOytfDpUxgR","timestamp":1571838737616}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"900dqb9wKoXJ","colab_type":"text"},"source":["# Multilayer perceptrons #\n","\n","You will frequently hear the term “MLP” to describe these models.\n","\n","It is “what it says on the box”:\n","\n","1. *Multiple* layers…\n","2. of stacked perceptrons\n","\n","Where each layer feeds into the next.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fBg8Ac4kE8jK","colab_type":"text"},"source":["## Back to logistic regression ##\n","\n","It is not uncommon to hear the term “Single-layer perceptron” for logistic regression, please do not use this term as it is endlessly confusing. If you absolutely have to, use the term “Single-layer MLP”, which while horrible in its own right is at least accurate as we will see in this lecture.\n","\n","$$ σ(x)  = \\frac{1}{1 + e^{-x}} $$\n","\n","$$ f({\\bf{x}}) = \\sigma(\\sum_{i}w_ix_i + b)=\\sigma({\\bf{w}\\cdot\\bf{x}}+b) $$\n","\n","![Schematic figure of a perceptron](https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png) \n","\n","[Image Source](https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png)"]},{"cell_type":"markdown","metadata":{"id":"a9Ztv7ynE_mn","colab_type":"text"},"source":["## From single to multi-layer perceptron ##\n","\n","![Schematic figure of a multi-Layer perceptron](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n","\n","[Image Source](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n","\n","We have a single re-usable component in single-layer perceptron (an **artificial neuron**), that we apply to each input ($\\sigma({\\bf{w}\\cdot\\bf{x}}+b)$) – remember the analogy to the human brain – forming a **layer**.\n","\n","Now, for the MLP we re-use not just the component itself, but rather the whole concept of a layer. Effectively constructing many different paths of information flow through a great **network** – again, remember the analogy to the human brain.\n","\n","This results in a powerful model that can describe non-linear functions (for example, the notorious XOR function)."]},{"cell_type":"markdown","metadata":{"id":"2HPsvpXNSQdG","colab_type":"text"},"source":["## Parallel perceptron cells ##\n","\n","In each layer, each neuron is executed in parallel, independent from each other neuron in the same layer.\n","\n","For $K$ parallel neurons in the first hidden layer:\n","\n","$$ h_1({\\bf{x}}) = \\sigma({\\bf{w_1}\\cdot\\bf{x}}+b_1) $$\n","\n","$$ h_2({\\bf{x}}) = \\sigma({\\bf{w_2}\\cdot\\bf{x}}+b_2) $$\n","\n","$$\\ldots$$\n","\n","$$ h_K({\\bf{x}}) = \\sigma({\\bf{w_K}\\cdot\\bf{x}}+b_K) $$\n"]},{"cell_type":"markdown","metadata":{"id":"_GR17_osIUvb","colab_type":"text"},"source":["### With matrix notation! ###\n","\n","We know that each $\\bf{w_k}\\cdot\\bf{x}$ is a vector (dot) product, with $\\bf{w_k}$,${\\bf{x}} \\in \\mathbb{R}^{n}$.\n","\n","Thus we can stack the weight vectors into a weight matrix:\n","\n","$$ W = [{\\bf{w_1}}, ..., {\\bf{w_K}}] \\in \\mathbb{R}^{n \\times K} $$\n","\n","Also, we can similarly stack the bias scalars into a bias vector:\n","\n","$$ {\\bf{b}} = [b_1, ..., b_K] \\in \\mathbb{R}^{K} $$\n","\n","This allows us to express the computation of a single layer neatly as:\n","\n","$$ h({\\bf{x}}) = \\sigma({W\\bf{x}}+{\\bf{b}}) \\in \\mathbb{R}^{K} $$\n","\n","where $\\sigma(.)$ is applied elementwise.\n","\n","We refer to $h({\\bf{x}})$ the **activations** of the hidden layer – again, remember the neural analogy. Ignoring the analogy to the human brain, mathematically a layer consists of an affine vector transformation of its input (in this case $\\bf{x}$), followed by the application of an element-wise, non-linear **activation** function $\\sigma$"]},{"cell_type":"markdown","metadata":{"id":"i6M_TgMlIXX5","colab_type":"text"},"source":["## Stacking into several layers ##\n","\n","- Above: parallel structure of several perceptrons \n","\n","- Now: stacking into multiple layers $\\rightarrow$ *multi-layer* perceptron\n","\n","We can recursively define the output of an $l$-layer multi-layer perceptron:\n","\n","$$ h_1({\\bf{x}}) = \\sigma({W_1\\cdot \\bf{x}}+{\\bf{b_1}}) $$\n","\n","$$ h_2({h_1({\\bf{x}})}) = \\sigma(W_2\\cdot h_1({\\bf{x}})+{\\bf{b_2}}) $$\n","\n","$$ \\ldots $$\n","\n","$$ h_l(h_{l-1}({\\bf{x}})) = \\sigma(W_l\\cdot h_{l-1}({\\bf{x}})+{\\bf{b_l}}) $$\n","\n","Where the final layer output $h_l$ can be used for classification/prediction, given an appropriate loss-function (such as the negative log-likelihood we used for logistic regression)."]},{"cell_type":"markdown","metadata":{"id":"D19osoIrIZLM","colab_type":"text"},"source":["## Why a non-linear activation function? ##\n","\n","Imagine an MLP without nonlinear activation (for the two-layer case):\n","\n","$$ f(x) = W_2 (W_1 x + b_1) + b_2 $$\n","\n","This can be re-written as a single affine transformation:\n","\n","$$ f(x) = W_2 W_1 x + (W_2 b_1 + b_2) = {\\tilde{W}}x + {\\tilde{b}} $$\n","\n","Where:\n","\n","$$ \\tilde{W} = W_2 W_1 $$\n","\n","$$ \\tilde{b} = (W_2 b_1 + b_2) $$\n","\n","We can clearly see that if the transformations can be summarised as a single affine transformation, we will only achieve a linear decision boundary – akin to the perceptron.\n","\n","Thus the expressive power of an MLP relies on its non-linearity to achivee non-linear decision boundaries. Making it more expressive than the perceptron/logistic regression. What we mean by “expressive” here, is that it can model more complex underlying data, for example, solve the XOR problem.\n","\n","But how does one train this **deep** network?"]},{"cell_type":"markdown","metadata":{"id":"hagAXRt6IbMr","colab_type":"text"},"source":["## Training ##\n","\n","As always, training involves finding concrete values for weights $W_1$, $W_2$, $\\ldots$, $W_l$, and biases $b_1$, $b_2,$ $\\ldots$, $b_l$ that fit a given dataset.\n","\n","We will use the same objective function here as for logistic regression, the log-likelihood loss ($L(x,y)$). After training, the network output $f(x)$ should represent probability for particular class.\n","\n","Logistic regression:\n","\n","$$ P_{\\theta}(y=1|x) = \\sigma(wx + b) $$\n","\n","MLP:\n","\n","$$ P_{\\theta}(y=1|x) = h_l({x}) $$\n","\n","Optimisation for will be done using the same method as for logistic regression: gradient descent. Which of course requires the gradients of the loss with respect to the model parameters. These gradients can be computed using partial derivatives and function composition."]},{"cell_type":"markdown","metadata":{"id":"K6aDUMdAdjns","colab_type":"text"},"source":["### Training via error backpropagation ##\n","\n","- Given input ${\\bf{x}}$, model prediction $h_K({\\bf{x}})$, and correct label $y$\n","- Recall negative log-likelihood error term $E(h_K({\\bf{x}}),y)$\n","- Case $y=1$: $E(h_K({\\bf{x}}),y)=-log(h_K({\\bf{x}}))$\n","- Case $y=0$: $E(h_K({\\bf{x}}),y)= - log(1-h_K({\\bf{x}}))$\n","\n","- How to minimize $E(h_K({\\bf{x}}),y)$ for a single data point? By changing model parameters such that its value decreases\n","- Gradient $\\nabla_{\\theta}E(h_K({\\bf{x}}),y)$ of error w.r.t all model parameters $\\theta$ points into steepest direction of decreasing error \n","- optimise model parameters $\\theta$ by following the gradient will decrease the model error\n","\n","![alt text](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png)\n","\n","[Image Source](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png)"]},{"cell_type":"markdown","metadata":{"id":"TfsOEZQgJXX7","colab_type":"text"},"source":["### Gradient derivations: Decomposition using the chain rule ###\n","\n","- Goal: find partial derivatives of the error w.r.t individual weights and biases\n","- Recall: error term is function of $h_K({\\bf{x}})$, which is in turn a result of many function compositions $h_K = \\sigma(W_{K-1}h_{K-1} + b_K)$;  ... $h_1({\\bf{x}})=W_1{\\bf{x}} + b_1$ \n","- Can compute how error changes depending on intermediate activations using chain rule\n","- Last Layer weights: $\\frac{\\partial E}{\\partial W_{K}} = \\frac{\\partial h_K}{\\partial W_{K}} \\frac{\\partial E}{\\partial h_{K}}$\n","- Last *last* layer weights: $\\frac{\\partial E}{\\partial W_{K-1}} = \\frac{\\partial h_{K-1}}{\\partial W_{K-1}}   \\frac{\\partial h_K}{\\partial h_{K-1}}  \\frac{\\partial E}{\\partial h_{K}}$\n","- Weights at arbitrary layer $k$:  $\\frac{\\partial E}{\\partial W_{k}} = \\frac{\\partial h_{k}}{\\partial W_{k}} \\frac{\\partial h_{k+1}}{\\partial h_{k}} ...   \\frac{\\partial h_K}{\\partial h_{K-1}}  \\frac{\\partial E}{\\partial h_{K}}$\n","\n","- Iterative application of chain rule for derivatives leads to chains of factors to compute gradients for weights in each layer\n","- Similarly for biases, e.g. in last layer $\\frac{\\partial E}{\\partial b_{K}} = \\frac{\\partial h_K}{\\partial b_{K}}\\frac{\\partial E}{\\partial h_{K}}$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"55kZRmTeJaIy","colab_type":"text"},"source":["### Individual derivative factors ##\n","\n","For simplification, let's only consider the case $y=1$ (the case $y=0$ analogous, but flipped minus sign, which implies a flipped gradient direction – recall the perceptron algorithm!):\n","\n","How does the error $E$ change depending on the last layer activation?\n","- $\\frac{\\partial E}{\\partial h_{K}}  = \\frac{\\partial}{\\partial h_K} ( -log(h_K)) = - \\frac{1}{h_K}$\n","\n","How do layer activations $h_{k}$ change depending on the previous layer? $\\frac{\\partial h_{k}}{\\partial h_{k-1}}$\n","- Define $a_k=W_{k}h_{k-1} + b_k$\n","- Recall: gradient of sigmoid activation: $\\sigma'(a) = \\sigma(a)[1-\\sigma(a)]$\n","- use chain rule for computing partial derivatives: $\\frac{\\partial h_{k}}{\\partial h_{k-1}}=\\frac{\\partial a_{k}}{\\partial h_{k-1}}  \\frac{\\partial h_{k}}{\\partial a_{k}}$\n","- $\\frac{\\partial a_{k}}{\\partial h_{k-1}}  = \\frac{\\partial}{\\partial h_{k-1}} [W_{k}h_{k-1} + b_{k}] = W_{k}$\n","- Putting it together: $\\frac{\\partial h_{k}}{\\partial h_{k-1}} = W_k \\cdot \\sigma(a_k)\\cdot [1-\\sigma(a_k)]$\n","\n","How do activations change depending on weights? $\\frac{\\partial h_k}{\\partial W_{k}}$\n","- $\\frac{\\partial h_k}{\\partial W_{k}} = \\frac{\\partial a_k}{\\partial W_{k}} \\frac{\\partial h_k}{\\partial a_{k}}$\n","- $\\frac{\\partial h_k}{\\partial a_{k}} = \\sigma(a_k)\\cdot[1-\\sigma(a_k)]$\n","- $\\frac{\\partial a_k}{\\partial W_{k}} = \\frac{\\partial}{\\partial W_k}[W_k h_{k-1} + b_k] = h_{k-1}$\n","\n","How do activations change depending on bias terms? $\\frac{\\partial h_k}{\\partial b_{k}}$\n","- analogous to above, but with $\\frac{\\partial a_k}{\\partial b_{k}} = \\frac{\\partial}{\\partial b_k}[W_k h_{k-1} + b_k]=1$\n","\n","These components, combined as factors using the chain rule, allow us to compute derivatives of the loss w.r.t arbitrary weights and biases in the network."]},{"cell_type":"markdown","metadata":{"id":"p4eA4oUlJczq","colab_type":"text"},"source":["## Backpropagation ##\n","\n","The derivative terms gets longer and longer as we reach earlier layers in the network. However, given the overlap between layers, all terms from subsequent layers are shared. Realising this, the efficient computation of *all* gradients is possible: starting with last layer (computing $\\frac{\\partial E}{\\partial h_{K}}$, and then computing individual factors $\\frac{\\partial h_{k+1}}{\\partial h_{k}}$ for each layer, decreasing $k$ in each step).\n","\n","Through this process, error information relative to the loss function **propagates** backward through the network. An intuitive view of this is that in order to understand how parameters affect the error (loss), we must find out how they affect subsequent activations, and how these affect then affect the error – rinse and repeat…"]},{"cell_type":"markdown","metadata":{"id":"wK1K7RDxJeic","colab_type":"text"},"source":["## Questions ##\n","\n","1. Consider the backpropagation expression: $\\frac{\\partial h_{k+1}}{\\partial h_{k}} = W_k \\cdot \\sigma(a_k)\\cdot [1-\\sigma(a_k)]$. When does it become large or small?\n","2. Where is the computational burden when computing gradients?\n","3. How would the situation change with different types of activation functions?\n","4. What is the total number of model parameters in an MLP?\n","5. How does the number of model parameters change with the size of each layer (assuming same size everywhere)?\n","6. How does the number of model parameters change with the number of layers?\n","7. What are dangers of using a very large model, and of using a very small model?\n","8. Can an MLP be used for a regression task? If so, what would have to change?"]}]}