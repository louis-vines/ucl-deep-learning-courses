{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"julia-1.0","display_name":"Julia 1.0"},"accelerator":"GPU","colab":{"name":"02.01 – A brief history of Machine Learning and Deep Learning","provenance":[{"file_id":"1iSOm24L3WOG4MBzv9xgyE_vG9IFQHBYA","timestamp":1599156198632}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"yehQNlXBEbsE","colab_type":"text"},"source":["# A brief history of Machine Learning and Deep Learning #\n","\n","![A portrait of a young Alan Turing](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Alan_Turing_Aged_16.jpg/352px-Alan_Turing_Aged_16.jpg)\n","\n","[Image source](https://commons.wikimedia.org/wiki/File:Alan_Turing_Aged_16.jpg)\n","\n","Neural networks, the basis of later deep learning research, really has two origins. One in the cognitive sciences from as early as the 1930s, where it was used as a model of processing in the brain, this line of research lives on to this day in computational neuroscience – for example the [Gatsby unit](http://www.gatsby.ucl.ac.uk) here at UCL. This line of research puts an emphasis on the biological plausability of the model.\n","\n","In parallel, early research in what we now call machine learning also sought to draw inspiration from “biological computation”. Traces of this line of research go as far back as Turing.\n","\n","## Rosenblatt’s perceptron ##\n","\n","![Frank Rosenblatt and a colleague adjusting the wiring of an early perceptron](https://csis.pace.edu/~ctappert/srd2011/photos/Rosenblatt-CAL1958.jpg)\n","\n","[Image source](http://csis.pace.edu/~ctappert/srd2011/rosenblatt-contributions.htm)\n","\n","![Mark 1 Perceptron internal wiring](https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg)\n","\n","[Image source](https://en.wikipedia.org/wiki/File:Mark_I_perceptron.jpeg)\n","\n","## Artificial Intelligence winters ##\n","\n","* 1970s\n","    - Machine translation failures\n","    - Speech recognition failures\n","    - Minsky and Pappert’s “Perceptrons”\n","* 1980s\n","    - Expert systems\n","    - [Fifth generation computers](https://en.wikipedia.org/wiki/Fifth_generation_computer)\n","\n","## Resurgence of statistical methods in the 90s ##\n","\n","![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/694px-Svm_separating_hyperplanes_%28SVG%29.svg.png)\n","\n","In the early 90s there is a resurgence of interest in statistical methods, lasting well into the mid-00s with non-neural feature-based methods such as linear classifiers and support vector machines.\n","\n","![](https://i2.wp.com/syncedreview.com/wp-content/uploads/2017/02/image3.png?resize=624%2C595&ssl=1)\n","\n","## Deep learning breakthrough points ##\n","\n","### ImageNet and Convolutional Neural Networks ###\n","\n","![Layers from a convolutional network](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-10-at-12-58-30-pm.png)\n","\n","![XXX](https://adriancolyer.files.wordpress.com/2016/04/imagenet-fig4l.png?w=600)\n","\n","![XXX](https://devblogs.nvidia.com/wp-content/uploads/2015/08/ILSVRC-Figure2-624x520.png)\n","\n","[Image source](https://devblogs.nvidia.com/nvidia-ibm-cloud-support-imagenet-large-scale-visual-recognition-challenge)\n","\n","### word2vec ###\n","\n","![XXX](https://samyzaf.com/ML/nlp/word2vec2.png)\n","\n","### Deep Reinforcement Learning ###\n","\n","![](https://raw.githubusercontent.com/kuz/DeepMind-Atari-Deep-Q-Learner/master/gifs/breakout.gif)\n","\n","![](https://cdn.vox-cdn.com/thumbor/sZluegyWYcFNXOrPElh385KK6p8=/0x78:640x438/1600x900/cdn.vox-cdn.com/uploads/chorus_image/image/49810381/deepmind_montezuma_s_revenge.0.0.gif)\n","\n","## End-of-lecture questions ##\n","\n","1. Backpropagation in its standard form is said not to be a plausible model of learning in the human brain, why do you think this is the case?\n","2. We are currently in an AI Summer, are there any indications or reasons for concern for another AI Winter?\n","\n","## Extracurricular reading ##\n","\n","1. [“Intelligent Machinery”](https://weightagnostic.github.io/papers/turing1948.pdf) by Turing (1948) is very accessible and gives a great picture of early AI thought and how he draws upon biological analogies.\n","2. [“Learning representations by back-propagating errors”](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf) by Rumelhart et al. (1986) is short and to the point, this was the work that brought back propagation into the spotlight.\n","3. The very opinionated and lengthy [“Deep Learning in Neural Networks: An Overview”](https://arxiv.org/abs/1404.7828) by Schmidhuber (2014) is an interesting read, even if I myself and many others disagree with his level of credit assignment."]}]}